
## Section Introduction
• Developing and performing AWS tasks against AWS can be done in several ways
    • Using the AWS CLI on our local computer
    • Using the AWS CLI on our EC2 machines
    • Using the AWS SDK on our local computer
    • Using the AWS SDK on our EC2 machines
    • Using the AWS Instance Metadata Service for EC2

## AWS CLI ON EC2... THE RIGHT WAY
• IAM Roles can be attached to EC2 instances
• IAM Roles can come with a policy authorizing exactly what the EC2 instance should be able to do
• EC2 Instances can then use these profiles automatically without any additional configurations
• This is the best practice on AWS and you should 100% do this.

## AWS CLI Dry Runs
• Sometimes, we’d just like to make sure we have the permissions...
• But not actually run the commands!
• Some AWS CLI commands (such as EC2) can become expensive if they
succeed, say if we wanted to try to create an EC2 Instance
• Some AWS CLI commands (not all) contain a --dry-run option to
simulate API calls

## AWS CLI STS Decode Errors
• When you run API calls and they fail, you can get a long error message 
• This error message can be decoded using the STS command line:
• *sts decode-authorization-message*

## AWS EC2 Instance Metadata
• AWS EC2 Instance Metadata is powerful but one of the least known features to developers
• It allows AWS EC2 instances to ”learn about themselves” without using an IAM Role for that purpose.
• The URL is http://169.254.169.254/latest/meta-data
• You can retrieve the IAM Role name from the metadata, but you CANNOT
 retrieve the IAM Policy.
• Metadata = Info about the EC2 instance
• Userdata = launch script of the EC2 instance

## MFA with CLI
• To use MFA with the CLI, you must create a temporary session
• To do so, you must run the STS GetSessionToken API call
• aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code code-from-token --duration-seconds 3600

## AWS SDK Overview
• We have to use the AWS SDK when coding against AWS Services such as DynamoDB
• Fun fact... the AWS CLI uses the Python SDK (boto3)
• The exam expects you to know when you should use an SDK
• We’ll practice the AWS SDK when we get to the Lambda functions
• Good to know: if you don’t specify or configure a default region, then us-east-1 will be chosen by default

## AWS Limits (Quotas)
• API Rate Limits
    • DescribeInstances API for EC2 has a limit of 100 calls per seconds
    • GetObject on S3 has a limit of 5500 GET per second per prefix 
    • For Intermittent Errors: implement Exponential Backoff
    • For Consistent Errors: request an API throttling limit increase

• Service Quotas (Service Limits)
    • Running On-Demand Standard Instances: 1152 vCPU
    • You can request a service limit increase by opening a ticket
    • You can request a service quota increase by using the Service Quotas API
  
## AWS CLI Credentials Provider Chain • The CLI will look for credentials in this order

1. Command line options – --region, --output, and --profile
2. Environment variables – AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_SESSION_TOKEN
3. CLI credentials file –aws configure
~/.aws/credentials on Linux / Mac & C:\Users\user\.aws\credentials on Windows
4. CLI configuration file – aws configure
~/.aws/config on Linux / macOS & C:\Users\USERNAME\.aws\config on Windows
5. Container credentials – for ECS tasks
6. Instance profile credentials – for EC2 Instance Profiles

## AWS SDK Default Credentials Provider Chain • The Java SDK (example) will look for credentials in this order
1. Java system properties – aws.accessKeyId and aws.secretKey
2. Environment variables – AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
3. The default credential profiles file – ex at: ~/.aws/credentials, shared by many SDK
4. Amazon ECS container credentials – for ECS containers
5. Instance profile credentials– used on EC2 instances

## AWS Credentials Best Practices
• Overall, NEVER EVER STORE AWS CREDENTIALS IN YOUR CODE
• Best practice is for credentials to be inherited from the credentials chain
• If using working within AWS, use IAM Roles 
    • => EC2 Instances Roles for EC2 Instances
    • => ECS Roles for ECS tasks
    • => Lambda Roles for Lambda functions
• If working outside of AWS, use environment variables / named profiles

## Signing AWS API requests
• When you call the AWS HTTP API, you sign the request so that AWS can identify you, using your AWS credentials (access key & secret key)
• Note: some requests to Amazon S3 don’t need to be signed
• If you use the SDK or CLI, the HTTP requests are signed for you
• You should sign an AWS HTTP request using Signature v4 (SigV4)

## S3 MFA-Delete
• MFA (multi factor authentication) forces user to generate a code on a device (usually a mobile phone or hardware) before doing important operations on S3
• To use MFA-Delete, enable Versioning on the S3 bucket
• You will need MFA to
    • permanently delete an object version 
    • suspend versioning on the bucket
• You won’t need MFA for 
    • enabling versioning
    • listing deleted versions
• *Only the bucket owner (root account) can enable/disable MFA-Delete*
• MFA-Delete currently can only be enabled using the CLI

## S3 Access Logs
• For audit purpose, you may want to log all access to S3 buckets
• Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket
• That data can be analyzed using data analysis tools...

## S3 Replication (CRR & SRR)
• Must enable versioning in source and destination
• Cross Region Replication (CRR)
• Same Region Replication (SRR)
• Buckets can be in different accounts
• Copying is asynchronous
• Must give proper IAM permissions to S3
• CRR - Use cases: compliance, lower latency access, replication across accounts
• SRR – Use cases: log aggregation, live replication between production and test accounts

## S3 Replication – Notes
• After activating, only new objects are replicated (not retroactive)
• For DELETE operations:
• Can replicate delete markers from source to target (optional setting)
• Deletions with a version ID are not replicated (to avoid malicious deletes)
• There is no “chaining” of replication
• If bucket 1 has replication into bucket 2, which has replication into bucket 3
• Then objects created in bucket 1 are not replicated to bucket 3

## S3 Pre-Signed URLs
• Can generate pre-signed URLs using SDK or CLI • For downloads (easy, can use the CLI)
• For uploads (harder, must use the SDK)
• Valid for a default of 3600 seconds, can change timeout with --expires-in [TIME_BY_SECONDS] argument
• Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT
• Examples :
• Allow only logged-in users to download a premium video on your S3 bucket
• Allow an ever changing list of users to download files by generating URLs dynamically • Allow temporarily a user to upload a file to a precise location in our bucket

## S3 Storage Classes
• Amazon S3 Standard - General Purpose
• Amazon S3 Standard-Infrequent Access (IA) 
• Amazon S3 One Zone-Infrequent Access
• Amazon S3 Intelligent Tiering
• Amazon Glacier
• Amazon Glacier Deep Archive


## Amazon Glacier & Glacier Deep Archive
• Amazon Glacier – 3 retrieval options: 
    • Expedited (1 to 5 minutes)
    • Standard (3 to 5 hours)
    • Bulk (5 to 12 hours)
    • Minimum storage duration of 90 days

• Amazon Glacier Deep Archive – for long term storage – cheaper:
    • Standard (12 hours)
    • Bulk (48 hours)
    • Minimum storage duration of 180 days

## S3 Lifecycle Rules
• Transition actions: It defines when objects are transitioned to another storage class.
    • Move objects to Standard IA class 60 days after creation 
    • Move to Glacier for archiving after 6 months
• Expiration actions: configure objects to expire (delete) after some time 
    • Access log files can be set to delete after a 365 days
    • Can be used to delete old versions of files (if versioning is enabled)
    • Can be used to delete incomplete multi-part uploads
• Rules can be created for a certain prefix (ex - s3://mybucket/mp3/*)
• Rules can be created for certain objects tags (ex - Department: Finance)

## S3 – Baseline Performance
• Amazon S3 automatically scales to high request rates, latency 100-200 ms
• Your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket.
• There are no limits to the number of prefixes in a bucket.
• Example (object path => prefix):
    • bucket/folder1/sub1/file => /folder1/sub1/ 
    • bucket/folder1/sub2/file => /folder1/sub2/ 
    • bucket/1/file => /1/
    • bucket/2/file => /2/
• If you spread reads across all four prefixes evenly, you can achieve 22,000 requests per second for GET and HEAD

## S3 – KMS Limitation
• If you use SSE-KMS, you may be impacted by the KMS limits
• When you upload, it calls the GenerateDataKey KMS API
• When you download, it calls the Decrypt KMS API
• Count towards the KMS quota per second (5500, 10000, 30000 req/s based on region)
• *You can request a quota increase using the Service Quotas Console*

## S3 Performance – S3 Byte-Range Fetches
• Parallelize GETs by requesting specific byte ranges
• Better resilience in case of failures 

Can be used to speed up downloads
Can be used to retrieve only partial data (for example the head of a file)

## S3 Select & Glacier Select
• Retrieve less data using SQL by performing server side filtering 
• Can filter by rows & columns (simple SQL statements)
• Less network transfer, less CPU cost client-side

## S3 Event Notifications
• S3:ObjectCreated,S3:ObjectRemoved, S3:ObjectRestore, S3:Replication...
• Object name filtering possible (*.jpg)
• Use case: generate thumbnails of images uploaded to S3
• Can create as many “S3 events” as desired
• S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer
• If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent
• If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.

## AWS Athena
• Serverless service to perform analytics directly against S3 files
• Uses SQL language to query the files
• Has a JDBC / ODBC driver
• Charged per query and amount of data scanned
• Supports CSV, JSON, ORC, Avro, and Parquet (built on Presto)
• Use cases: Business intelligence / analytics / reporting, analyze & query
VPC Flow Logs, ELB Logs, CloudTrail trails, etc...
• Exam Tip: Analyze data directly on S3 => use Athena