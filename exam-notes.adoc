- Amazon EC2 Auto Scaling works with both Application Load Balancer and Network Load Balancer
- Amazon EC2 Auto Scaling cannot add a volume to an existing instance if the existing volume is approaching capacity.

- In the *ecs.config* file you have to configure the parameter ECS_CLUSTER='your_cluster_name' to register the container instance with a cluster named 'your_cluster_name'.

- Use Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects 
- Use Resource-based Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects
- Use Cross-account IAM roles for programmatic and console access to S3 bucket objects
- Stage Variables: Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of an API. They act like environment variables and can be used in your API setup and mapping templates.

- Amazon S3: If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent.

- Which section of cloud formation does not allow condition: parameters

- SQS max message size is 256KB.

- member accounts will be able to see the Organization trail, but cannot modify or delete it.
- by default, CloudTrail tracks only bucket-level actions, to track object-level actions, you need to enable Amazon S3 data events.
- A multi-national company maintains separate AWS accounts for different verticals in their organization. The project manager of a team wants to migrate the Elastic Beanstalk environment from Team A's AWS account into Team B's AWS account. As a Developer, you have been roped in to help him in this process: 
    -- create a saved configuration in Team A's account and download it to your local machine. Make the account-specific parameter changes and upload to the S3 bucket in Team B's account. From Elastic Beanstalk console, create an application from 'Saved Configurations'

- To use the exported value in another stack, use *!ImportValue*

!Ref - Returns the value of the specified parameter or resource.

!GetAtt - Returns the value of an attribute from a resource in the template.

!Sub - Substitutes variables in an input string with values that you specify.

!Join - This function appends a set of values into a single value, separated by the specified delimiter

- If you upload a local template file, AWS CloudFormation uploads it to an Amazon Simple Storage Service (Amazon S3) bucket in your AWS account. If you don't already have an S3 bucket that was created by AWS CloudFormation, it creates a unique bucket for each region in which you upload a template file

- CodeDeploy Deployment Groups : You can specify one or more deployment groups for a CodeDeploy application. The deployment group contains settings and configurations used during the deployment

- You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your lambda function to the VPC to access private resources during execution. When you connect a function to a VPC, Lambda creates an elastic network interface for each combination of the security group and subnet in your function's VPC configuration

- Consider using Global tables if your application is accessed by globally distributed users - If you have globally dispersed users, consider using global tables. With global tables, you can specify the AWS Regions where you want the table to be available. This can significantly reduce latency for your users.

- During rollback which of the following instances did AWS CodeDeploy deploy first to: failed instances

- If you terminate a container instance while it is in the STOPPED state, that container instance isn't automatically removed from the cluster. You will need to deregister your container instance in the STOPPED state by using the Amazon ECS console or AWS Command Line Interface. Once deregistered, the container instance will no longer appear as a resource in your Amazon ECS cluster.

- By default, the health check configuration of your Auto Scaling group is set as an EC2 type that performs a status check of EC2 instances. To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file.

- by default, scripts entered as user data are executed with root user privileges
- by default, user data runs only during the boot cycle when you first launch an instance

- assign permissions to services using IAM roles

- iam credentials report (account level), iam access advisor (user level)

- ec2 instance connect works only out of the box with Amazon Linux 2

- by default only the root EBS volume is deleted, any other attached EBS volume is not deleted

- not necessary to detach volume to do snapshot, but recommended.

- can copy snapshots across AZ or region

- AMI are built for a specific region (and like EBS, it can be copied across regions)

- EBS volume type: gp2/gp3 (general ssd), io1/io2 (high performance ssd), st1 (low cost hdd), sc1 (lowest cost hdd)

- only gp2/gp3 and io1/io2 can be used as boot volumes

- gp2/gp3 : PIOPS (Provisioned IOPS) increase along side with storage size
- io1/io2 : PIOPS can increase independently with storage size -> for applications that need more than 16000 IOPS

- *io1/io2 can be attached to multple EC2 instances in the same AZ*.

- uses security group to control access to EFS

- EFS is only compatible with Linux based AMI (not windows)

- EFS-IA (infrequently access)

- EBS backups use IO and you shouldn't run them while your application is handling a lot of traffic.

- you can setup private or public ELB

- ALB (application load balancer) supports load balancing to multiple applications on the same machine (containers), support Websocket and HTTP/2

- ALB has a port mapping feature to redirect to a dynamic port in ECS

- In comparison, we’d need multiple Classic Load Balancer per application

- ALB can route to multiple target groups, health checks are at the target group level

- NLB (Network Load Balancer) has one static IP per AZ , and supports assigning Elastic IP.

- stickiness works for CLB and ALB

- cross-zone load balancing: requests are distributed evenly across all registered instances in all AZ.

- cross-zone load balancing always on for ALB, enable by default through console for classic load balancer, disable by default for network load balancer

- load balancer use X.509 certificate

- SNI (Server Name Indication) to support multiple certs for multiple domains

- SNI solves the problem of loading multiple SSL certificates onto one web server, it's a "newer" protocol and requires the client to indicate the hostname of the target server in the initial SSL handshake

- SNI only works for newer genration load balancer (ALB, NLB)

- Connection draining (CLB) , deregistration delay (ALB, NLB) : time to complete in-flight requests while the instance is de-registering or unhealthy

- it's possible to scale an ASG based on CloudWatch Alarm

- it's now possible to define "better" auto scaling rules that are directly managed by EC2.

- ASG can terminate instances that are marked as unhealthy by LB (hence replace them)

- the cooldown period helps to ensure that your Auto Scaling group doesn't launch or terminate additional instances before the previous scaling activity takes effect.

- In AWS there’s a network cost when data goes from one AZ to another

- RDS read replica: async, RDS disaster recovery: sync

- If the master is not encrypted, the read replicas cannot be encrypted

- IAM-based authentication can be used to login into RDS MySQL & PostgreSQL (Aurora )

- RDS encryption at rest is done only when you first create the DB instance

- aurora writer endpoint, reader endpoint

• In AWS, the most common records are: 
• A: hostname to IPv4
• AAAA: hostname to IPv6
• CNAME: hostname to hostname (only works with non root domain)
• Alias: hostname to AWS resource. (works with root and non root domain)

- if multiple values are returned, a random one is chosen by the client

- NAT gateways & NAT Instances allow your instances in your Private Subnets to access the internet while remaining private

- Endpoints allow you to connect to AWS Services using a private network instead of the public www network

- buckets must have a globally unique name

- S3 max object size is 5TB, upload more than 5GB must use multipart upload

- any files that is not versioned prior to enabling versioning will have version "null"

- suspending versioning does not delete the previous versions

- There are 4 methods of encrypting objects in S3:
    + SSE-S3: keys handled and managed by S3, Must set header: “x-amz-server-side-encryption": "AES256"
    + SSE-KMS: keys handled and managed by KMS, Must set header: “x-amz-server-side-encryption": ”aws:kms"
    + SSE-C: key is managed by client, HTTPS must be used
    + Client side encryption

- User based: IAM
- Resources based: Bucket policies, Object access control list, bucket access control list
- use s3 bucket policy to :
    + grant public access to the bucket
    + force objects to be encrypted at upload
    + grant access to another account

- s3 access logs can be stored in other S3 bucket

• <bucket-name>.s3-website-<AWS-region>.amazonaws.com

- S3 strong consistency as of Dec 2020.

- never ever put your credentials on an EC2 using `aws configure`, assign IAM roles to EC2 instances instead.

- use --dry-run to check if we have the permissions to run the command.

- when you run API calls and they fail, you can get a long error message
- this error message can be decoded using the STS commandline: *sts decode-authorization-message*

- EC2 Instance metadata: allows EC2 to learn about themselves without using an IAM role. http://169.254.169.254/latest/meta-data

- to use MFA with CLI, you must run STS GetSessionToken API call
- aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code code-from-token --duration-seconds 3600

- when use SDK, if region is not set, it is *us-east-1* by default.

- You can request a service limit increase by opening a ticket

- You can request a service quota increase by using the Service Quotas API

- AWS CLI credentials chain:
    + command line options
    + environment variables
    + ~/.aws/credentials -> ~/.aws/config
    + container credentials (ec2 tasks)
    + instance profile credentials (ec2 instance profiles)

- AWS HTTP request should use Signature V4 (SigV4) for signing.

- If you use the SDK or CLI, the HTTP requests are signed for you.

- to use MFA, enable versioning on the S3 bucket.

- Only the bucket owner (root account) can enable/disable MFA-Delete

- you will need MFA to: permanently delete an object version, suspend versioning on the bucket.

- S3 replication is async, bucket can be in different regions, accounts.

- there is no chaining of replication, if bucket 1 has replication into bucket 2, which has replication into bucket 3, then objects created in bucket 1 are not replicated to bucket 3

- each item in Glacier is called Archive, archives are stored in Vaults

- 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket

- If you use SSE-KMS, you may be impacted by the KMS limits

-  When you upload, it calls the GenerateDataKey KMS API, When you download, it calls the Decrypt KMS 

- S3 transfer acceleration: Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region

- S3 Byte-range fetch: Can be used to retrieve only partial data, Can be used to speed up downloads

- s3 select, glacier select: retrieve less data using SQL by performing server side filtering (simple SQL statements)

- S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer
- If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent
- If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.

- AWS Athena: Serverless service to perform analytics directly against S3 files, Exam Tip: Analyze data directly on S3 => use Athena

- CloudFront origins: S3, ALB, EC2, S3 website, any HTTP backend you want

- CloudFront geo restriction: whitelist, blacklist

- Cloudfront: greate for static content that must be available everywhere
- S3 CRR : greate for dynamic content that needs to be available at low-latency in few regions.
- You can invalidate part of the cache using the CreateInvalidation API

- viewer protocol policy: redirect HTTP to HTTPS
- origin policy: match viewer (http - http, https - https) or https only
- s3 bucket websites dont support https.

- signed url: individual files
- signed cookies: multiple files

- in your cloudfront distribution, create one or more trusted key groups
- you generate your own private/public keys:
    + private key will be used by your application to sign URLs
    + public key will be used by CloudFront to verify URLs

- three price classes: all, 200, 100

- cloudfront origin groups: increase high-availability and do failover
- origin group: one primary and one secondary
- if the primary fails, the second one is used

- cloudfront field level encryption: uses asymmetric encryption

- the EC2 instances run a special AMI made specifically for ECS, it runs an ECS agent to reguster the instace with ECS cluster
- ECS task: tell ECS how to run a docker container
- ECS service: define how many tasks should be run and how they should be run   

- ECS service with LB and dynamic port forwarding

- AWS CLI v1 login command : $(aws ecr get-login --no-include-email --region eu-west-1)
- AWS CLI v2 login command : aws ecr get-login-password --region eu-west-1 | docker login --username AWS -- password-stdin 1234567890.dkr.ecr.eu-west-1.amazonaws.com

- Fargate: serverless, dont manage and provision EC2 instances, only need to create task definitions

- EC2 instance profile: used by ECS agent 
- ECS Task Role: allow each task to have specific role, use different roles for different service.

- ECS task placement strategy
    + instances match spec -> match constraints -> match strategy -> select instances
    + *distinctInstance*: place each task on a different container instance
    + *memberOf*: place task on instance satisfy an expression

- ECS task placement constraints

- ECS service scaling != EC2 Auto Scaling

- A Capacity Provider is used in association with a cluster to determine the infrastructure that a task runs on
- Fargate: FARGATE and FARGATE_SPOT capacity providers are added automatically
- ECS: you need to associate the capacity provider with an auto-scaling group

+ ECS + EBS: cant be shared
+ ECS + EFS: can be shared
+ Bind mounts: share data between containers of the same task, Great for “sidecar” container pattern where the sidecar can be used to send metrics/logs to other destinations

- We must configure the file */etc/ecs/ecs.config* with the cluster name
- In case an EC2 instance (or you) cannot pull a Docker image, check IAM
- Fargate tasks can have IAM Roles to execute actions against AWS

- Beanstalk is free but you pay for the underlying instances
- Beanstalks: application (with version) and environment name

- support multiple platforms: go, java, python,... if not supported, you can write your custom platform.

- beanstalk deployment options: all at once, rolling, rolling with additional batches, immutables (deployment time is sorted in ascending order, immutables is the longest one)

- blue/green deployment with route53 weighted policy and beanstalk's swap URL

- deployment process: describe dependency, zip code -> beanstalk will deploy zip code to ec2 instances, resolve dependencies and start the application

- beanstalk lifecycle policy: max 1000 app -> need to delete some, can set policy to delete, based on time (old versions) or space (too many versions)

- beanstalk extensions: *.ebextensions* directory in the root of source code, resources managed by `.ebextensions` get deleted if the environment goes away

- you can define CloudFormation resources in your .ebextensions to provision ElastiCache, an S3 bucket, anything you want

- beanstalk environment cloning, cannot clone load balancer though, so we have to do a trick by creating new environment and use a cname swap with route53

- beanstalk single docker: Dockerrun.aws.json (describe where the built docker image is)
- beanstalk multi docker: required Dockerrun.aws.json at the root of source code, it is used to generate ECS task definition

- beanstalk HTTPS:  .ebextensions/securelistener-alb.config

- beanstalk custom platform: define platform.yaml file and build the platform using Packer software (opensource tool for creating AMI)

- codepipeline: made of stages, each stage can have sequential actions and or parallel actions

- manual approval can be defined at any stage

- aws code deploy -> deploy to fleet of EC2 instances (not beanstalk, code pipeline can be deployed to code deploy or beanstalk)

- each pipeline stage create an artifact, artifact go to S3 and be input of next stage

- codebuild instruction is defined in *buildspec.yaml* file, must be at the root of your code
    + 4 phases: install, prebuild, build, postbuild -> artifacts (to S3)

- codebuild can be run locally for troubleshooting

- builds can be defined within codepipeline or codebuild itself.

- by default, codebuild are launched outside your VPC -> cannot access resources in a VPC -> you can specify a VPC configuration

- code deploy : *appspec.yaml*, ec2 machine must be running code deploy agent

- EC2 instances are grouped by deployment group

- code deploy does not provision resources

- code deploy appspec.yaml hooks: App stop, download bundle, before install, after install, app start, validate service

- code deploy failed: instances will be at failed state, new deployments will first be deployed to failed state instances

- If a roll back happens, CodeDeploy redeploys the last known good revision as a new deployment.

- cloudformation stacks: separate of concern, vpc stacks, network stacks, app stacks

- cloudformation templates have to be uploaded in S3 and then referenced in cloudformation

- cant upload a cloudformation template, have to create a new one

- cloudformation template components: 
    + resources: aws resources (MANDATORY)
    + parameters: dynamic inputs
    + mapping: static variables
    + outputs: declares optional outputs values that we can import into other stacks (if you export them first)
    + conditions: used to control the creation of resources or outputs based on a condition
    + metadata


- aws resources form: AWS::aws-product-name::data-type-name

- you cant delete a cloudformation stack if its outputs are being referenced by another cloudformation stack

- !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]

- Fn::GetAtt : get attributes attached to any resources you create

- nested stack: separate common components

- cloudformation stackset: CRUD for stacks across multiple accounts and regions with a single operation

- cloudformation drift: if you want to know if the infrastructure is changed manually, use cloudformation drift

- metrics have dimension (up to 10 dimension)

- EC2 instance metrics have metrics every 5 minutes, with detailed monitoring, you get data every minute

- EC2 memory usage is by default not pushed

- cloudwatch custom metric resolution, standard: 1 minute, high resolution: 1 second

- cloudwatch alarm high resolution: 10 or 30 sec

- by default, no logs from your EC2 machine will go to cloudwatch, you need to run a cloudwatch agent on EC2 to push the log files you want

- cloud watch log agent & unified agent

- cloud watch logs can use filter expression, metrics filter can be used to trigger alarms.

- cloudwatch events / event bridge: event bridge is the next evolution of cloudwatch events,
    + default event bus: aws services
    + partner event bus: 
    + custom event bus:

- EventBridge allows extension to add event buses for your custom applications and your third-party SaaS apps.

- AWS X-Ray Visual analysis of our applications

- X-ray daemon: low level UDP interceptor 

- By default, the X-Ray SDK records the first request each second, and five percent of any additional requests.

- X-Ray write apis: PutTraceSegments, PutTelemetryRecords, GetSamplingRules

- X-Ray read apis: GetServiceGraph, BatchGetTraces, GetTraceSummaries, GetTraceGraph

- beanstalk x-ray config: *.ebextensions/xray-daemon.config*

- Can put logs from CloudTrail into CloudWatch Logs or S3

- A trail can be applied to all regions (default) or a single region

- cloud trail events:
    + management events
    + data events: By default, data events are not logged
    + cloudtrail insights events: detect unusual activity

- Events are stored for 90 days in CloudTrail
- To keep events beyond this period, log them to S3 and use Athena

- SQS max 14 retention days

- can have duplicate and out of order messages

- SQS multiple consumers: consume in parallel, consumers delete message after processing them, scale consumer horizontally to increase throughput

- SQS Access policy: cross account access, allow other services to write to an SQS queue

- after a message is polled by a consumer, it becomes invisible to other consumers
- by default, message visibility timeout is 30 seconds
- after MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ)

- sqs delay: delay a message up to 15 minutes before the consumers can see it (default is 0 seconds)

- long polling: wait time can be 1 to 20 seconds

- if message > 256kb, use sqs extended client (java library)

- sqs fifo queue: limitied throughput, exactly-once send, messages are processed in order by the consumer

- sqs fifo deduplication: 5 minutes interval, dedup by id or by content

- messages that share a common message group id will be in order within the group

- s3 events to multiple queue (sns + sqs fan out)

- sns fifo (similar to sqs fifo): ordering + deduplicating, *can only have sqs fifo as subscriber*

- sns message filtering with filter policy: subscriber only sees what they want

- Kinesis Data Streams: capture, process, and store data streams
- Kinesis Data Firehose: load data streams into AWS data stores
- Kinesis Data Analytics: analyze data streams with SQL or Apache Flink
- Kinesis Video Streams: capture, process, and store video streams

- Once data is inserted in Kinesis, it can’t be deleted

- Data that shares the same partition goes to the same shard

- kinesis consumer: 
    + classic fan-out consumers: 2MB/sec across all consumers, consumers pull data from kinesis
    + enhanced fan-out consumers: 2MB/sec each consumer, kinesis pushs data to consumers
- kinesis consumer lambda: read records in batch
- kinesis client library: java libray, 1 shard = 1 kcl
- kinesis shards splitting : to increase throughput because 1 shard has 1mb/s data
- cant split into more than two shards in a single operation
- kinesis shards merging: save costs, cant merge more than 2 shards in a single operation 

- kinesis data firehose: near real time

- kinesis data analytics: real time analytics on Kinesis Stream using SQL

- ordering data into kinesis using `partition key` (like group id in sqs)

- Serverless is a new paradigm in which the developers don’t have to manage servers anymore...

- AWS lambda 10gb ram max, ram increase -> cpu increase

- lambda synchronous invoke: ELB, API Gateway, Cloudfront, Cognito, Step Functions
    Kinesis Data Streams, SQS & SQS FIFO, DynamoDB Streams
        + event source mapping
- lambda asynchronous invoke: 
    S3, SNS, cloudwatch event/eventbridge
        + events are placed in `event queue`


- HTTP headers and query string parameters that are sent with multiple values are shown as arrays within the AWS Lambda event and response objects.

- Lambda@Edge, used with cloudfront, for example to filter request before reaching resources

(kinesis and dynamoDB)
- stream & lambda: By default, if your function returns an error, the entire batch is reprocessed until the function succeeds, or the items in the batch expire.
- To ensure in-order processing, processing for the affected shard is paused until the error is resolved

- When you use an event source mapping to invoke your function, Lambda uses the execution role to read event data.

- Use resource-based policies to give other accounts and AWS services permission to use your Lambda resources

An IAM principal can access Lambda:
    • if the IAM policy attached to the principal authorizes it (e.g. user access) 
    • OR if the resource-based policy authorizes (e.g. service access)

- Lambda Environment Variables

- Make sure your AWS Lambda function has an execution role with an IAM policy that authorizes writes to CloudWatch Logs

- Enable in Lambda configuration (Active Tracing). lambda x-ray
Environment variables to communicate with X-Ray
- _X_AMZN_TRACE_ID: contains the tracing header
- AWS_XRAY_CONTEXT_MISSING: by default, LOG_ERROR
- AWS_XRAY_DAEMON_ADDRESS: the X-Ray Daemon IP_ADDRESS:PORT

- by default, lambda function is launched outside VPC => we need to use VPC endpoints so that lambda function can access other AWS resources
- but a Lambda function in a VPC still cannot access internet, Deploying a Lambda function in a public subnet does not give it internet access or a public IP
=> deploy lambda in a private subnet and give it internet access with NAT Gateway/Instance

- Lambda Execution Context: is a temporary runtime environment that initializes any external dependencies of your lambda code
- the execution context is maintained for sometime in anticipation of another lambda function invocation
- execution context includes `/tmp` directory, it has capacity of 512mb
- for permanent persistent, use S3

- up to 1000 concurrent execution,
- can set a "reserved concurrency" at the function level
    + sync invoke: 429
    + async invoke: auto retry, then to DLQ
- if you need higher limit, open a support ticket

- if the dependencies is large, zip it and upload to lambda if < 50MB, or else S3

- AWS SDK comes by default with every lambda function

- lambda inline cloudformation : declare lambda function directly in cloudformation json file

- lambda layers: externalize dependencies to reuse them

- lambda container image: deploy lambda as container images
- can creat your own image as long as it implements the lambda runtime API
- Test the containers locally using the Lambda Runtime Interface Emulator

- When you work on a Lambda function, we work on $LATEST
- version can be associated with alias, alias enable blue/green deployment by assigning weights to lambda function

- CodeDeploy can help you automate traffic shift for lambda function
    + linear: grow traffic every N minutes until 100%
    + Canary: try X percentages then 100%
    + AllAtOnce: immediate

- max size of an item in dynamoDB is 400kb

- dynamoDB primary key: partition key only (hash) or partition key + sort key

- Read Capacity Units (RCU):
    + Strongly consistent: 1 RCU = 1 strongly consistent read of 4KB
    + Eventually consistent (by default):  1 RCU = 2 eventually consistent read of 4KB

- Write Capacity Units (WCU):
    + 1 WCU = 1 kb / sec


- WCU and RCU are spread evenly between partitions

- Writing data: PutItem, UpdateItem, ConditionalWrites
- Deleting data: DeleteItem, DeleteTable
- Reading data: GetItem, BatchGetItem (ProjectionExpression to get only attributes we want)

- dynamoDB query & scan: query is more efficient and faster, scan will scan the entire table
- Local Secondary Index (LSI):
    + 5 LSI per table
    + must be defined at table creation time

- Global Secondary Index (GSI): the index is a new table and we can project attributes on it
    + must define RCU/WCU for the index
    + can add/modify GSI (not LSI)

- If the writes are throttled on the GSI, then the main table will be throttled!

- dynamoDB has a feature called "conditional update/delete", that means you can ensure an item has not changed before altering it
    => optimistic locking / concurrency database

- Writes go through DAX to DynamoDB

- Changes in DynamoDB (Create, Update, Delete) can end up in a DynamoDB 

- DynamoDB Streams:
    + KEYS_ONLY
    + NEW_IMAGE
    + OLD_IMAGE
    + NEW_AND_OLD_IMAGES

- DynamoDB Streams are made of shards, just like Kinesis Data Streams

- DynamoDB TTL: 
    + automatically delete an item after an expiry date / time
    + do not use RCU/WCU because TTL is a background task operated by the DynamoDB service itself

- DynamoDB Transactions
    + Transaction = Ability to Create / Update / Delete multiple rows in different tables at the same time
    + Consume 2x of WCU / RCU

- DynamoDB as Session State Cache

- DynamoDB Write Sharding: add random suffix 

- large object goes to S3 

- Making changes in the API Gateway does not mean they’re effective
- You need to make a “deployment” for them to be in effect
- Changes are deployed to “Stages” 

- Stage variables are like environment variables for API Gateway

- API Gateway – Canary Deployment

- API Gateway - IntegrationTypes:
    + MOCK: return response without calling to the backend
    + HTTP / AWS: mapping templates for the request & response
    + AWS_PROXY: incoming request from the client is the input to Lambda, No mapping template, headers, query string parameters... are passed as arguments
    + HTTP_PROXY

- Mapping Templates Uses Velocity Template Language : for loop, if etc...

- Caches are defined per stage, possible to override cache settings per method

- Clients can invalidate the cache with header: Cache-Control: max-age=0

- API keys: If you want to make an API available as an offering ($) to your customers

- API Gateway throttles requests at10000 rps across all API

- Just like Lambda Concurrency, one API that is overloaded, if not limited, can cause the other APIs to be throttled

- API Gateway resource policies

- API Gateway Cognito User Pools 

- API Gateway – Security Lambda Authorizer 

- WebSocket URL wss://[some-uniqueid].execute-api.[region].amazonaws.com/[stage-name]

- AWS Serverless Application Model (SAM): Framework for developing and deploying serverless applications

• Transform Header indicates it’s SAM template: Transform: 'AWS::Serverless-2016-10-31'
• Write Code
    • AWS::Serverless::Function
    • AWS::Serverless::Api
    • AWS::Serverless::SimpleTable
• Package & Deploy:
    • aws cloudformation package / sam package
    • aws cloudformation deploy / sam deploy

- SAM framework natively uses CodeDeploy to update Lambda functions

• SAM is built on CloudFormation
• SAM requires the Transform and Resources sections
• Commands to know:
    • sam build: fetch dependencies and create local deployment artifacts
    • sam package: package and upload to Amazon S3, generate CF template 
    • sam deploy: deploy to CloudFormation
• SAM Policy templates for easy IAM policy definition
• SAM is integrated with CodeDeploy to do deploy to Lambda aliases

- Serverless Application Repository (SAR):  Managed repository for serverless applications, The applications are packaged using SAM

- Cognito has a hosted authentication UI that you can add to your app to handle sign- up and sign-in workflows

- Cognito Identity Pools (Federated Identities): Get identities for “users” so they obtain temporary AWS credentials

- IAM credentials are obtained by Cognito Identity Pools through STS

- AWS Step Functions: Model your workflows as state machines (one per workflow)
- States:
    + choice state:
    + fail or succeed state: 
    + pass state:
    + wait state:
    + map state:
    + parallel state:

- Use Retry (to retry failed state) and Catch (transition to failure path) in the State Machine to handle the errors instead of inside the Application Code
    + Retry: evaluate from top to bottom 
        • ErrorEquals: match a specific kind of error
        • IntervalSeconds: initial delay before retrying
        • BackoffRate: multiple the delay after each retry
        • MaxAttempts: default to 3, set to 0 for never retried
        • When max attempts are reached, the Catch kicks in
    + Cache: evaluate from top to bottom
        • ErrorEquals: match a specific kind of error
        • Next: State to send to    
        • ResultPath - A path that determines what input is sent to the state specified in the Next field.

- Step Functions: Standard & Express

• AppSync is a managed service that uses GraphQL
• GraphQL makes it easy for applications to get exactly the data they need.
• This includes combining data from one or more sources

• Allows to grant limited and temporary access to AWS resources (up to 1 hour). 
• AssumeRole: Assume roles within your account or cross account
• AssumeRoleWithSAML: return credentials for users logged with SAML
• AssumeRoleWithWebIdentity
    • return creds for users logged with an IdP (Facebook Login, Google Login, OIDC compatible...) 
    • AWS recommends against using this, and using Cognito Identity Pools instead
• GetSessionToken: for MFA, from a user or AWS account root user
• GetFederationToken: obtain temporary creds for a federated user
• GetCallerIdentity: return details about the IAM user or role used in the API call
• DecodeAuthorizationMessage: decode error message when an AWS API is denied

- Using STS to Assume a Role: Temporary credentials can be valid between 15 minutes to 1 hour

IAM Best Practices – IAM Roles
• EC2 machines should have their own roles
• Lambda functions should have their own roles
• ECS Tasks should have their own roles (ECS_ENABLE_TASK_IAM_ROLE=true)
• CodeBuild should have its own service role
• Create a least-privileged role for any service that requires it
• Create a role per application / lambda function

Advanced IAM - Authorization Model
Evaluation of Policies, simplified
1. If there’s an explicit DENY, end decision and DENY
2. If there’s an ALLOW, end decision with ALLOW
3. Else DENY

Dynamic Policies with IAM: Leverage the special policy variable ${aws:username}

To configure many AWS services, you must pass an IAM role to the service
The service will later assume the role and perform actions
For this, you need the IAM permission *iam:PassRole*

- Roles can only be passed to what their trust allows

AWS Directory Services:
    + AWS Managed Microsoft AD
    + AD Connector: proxy to on premise AD
    + Simple AD

KMS – Customer Master Key (CMK)Types
    + Symmetric (AES-256 keys)
    + Asymmetric (RSA & ECC key pairs): Use case: encryption outside of AWS by users who can’t call the KMS API

- KMS can only help in encrypting up to 4KB of data per call
- If Data > 4KB, use envelope encryption

• For the exam: anything over 4 KB of data that needs to be encrypted must use the Envelope Encryption == GenerateDataKey API

- S3 Bucket Key for SSE-KMS encryption, save costs, less calls to KMS

- SSM Parameter Store: Secure storage for configuration and secrets

- AWS Secrets Manager: Newer service, meant for storing secrets, Mostly meant for RDS integration

- cloudwatch log encryption: You cannot associate a CMK with a log group using the CloudWatch console, you must use the CloudWatch Logs API.

- AWS Certificate Manager (ACM):  host public SSL certificates in AWS

- AWS SWF – Simple Workflow Service: Coordinate work amongst applications
    Step Functions is recommended to be used for new applications, except: 
    • If you need external signals to intervene in the processes
    • If you need child processes that return values to parent processes

- AWS SES – Simple Email Service

------ TEST 1 ------

- AWS requires approximately 5 weeks of usage data to generate budget forcast.

- exported output values in cloudformation must have unique name within a single region

- elastic beanstalk will replace the failed instances with instances running the application version from the most recent successful deployment.

- CodeCommit credential types:
    + SSH Keys
    + Git credentials
    + AWS Access keys

- Auto Scailing Group cannot span across multiple region
- An ASG can contain EC2 in one or more AZ within a region
- Amazon EC2 Auto Scaling attempts to distribute instances evenly between the Availability Zones that are enabled for your Auto Scaling group

- By default, IAM users do not have access to the AWS billing account and cost management, you can do this by activating IAM user access to the Billing and Cost Management console. 

- Sampling rules tell the X-Ray SDK how many requests to record for a set of criteria.

- The X-ray daemon logs could help with figuring out the problem

- Elastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer.

Some policies replace all instances during the deployment or update. This causes all accumulated Amazon EC2 burst balances to be lost. It happens in the following cases:
    Managed platform updates with instance replacement enabled
    Immutable updates
    Deployments with immutable updates or traffic splitting enabled

- AWS states that, if your AWS account is less than 12 months old, you can use a t2.micro instance for free within certain usage limits.

- *A Load Balancer can target EC2 instances only within an AWS Region.*

- These two policies only limit permission but cant grant permission:
    + AWS Organizations Service Control Policy (SCP)
    + Permission boundary

- AWS IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, that are shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk.

-  IAM is used as a certificate manager only when you must support HTTPS connections in a Region that is not supported by ACM.

- Access Advisor feature on IAM console- To help identify the unused roles, IAM reports the last-used timestamp that represents when a role was last used to make an AWS request. Your security team can use this information to identify, analyze, and then confidently remove unused roles.


------ TEST 2 ------

- By default, basic monitoring is enabled when you create a launch template or when you use the AWS Management Console to create a launch configuration for ASG.

- Signed URLs take precedence over signed cookies. If you use both signed URLs and signed cookies to control access to the same files and a viewer uses a signed URL to request a file

- A Task state ("Type": "Task") represents a single unit of work performed by a state machine. 
- *Resource* field is a required parameter for Task state.

- However, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects. The pre-signed URLs are valid only for the specified duration.

- Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), and Splunk.

- You can specify that Amazon EC2 should do one of the following when it interrupts a Spot Instance:
    Stop the Spot Instance
    Hibernate the Spot Instance
    Terminate the Spot Instance
    The default is to terminate Spot Instances when they are interrupted.

- When you purchase a Reserved Instance for a Region, it's referred to as a regional Reserved Instance. A regional Reserved Instance does not provide a capacity reservation. But a Zonal Reserved Instance does.

- When you create a signer, the public key is with CloudFront and private key is used to sign a portion of URL - Each signer that you use to create CloudFront signed URLs or signed cookies must have a public–private key pair. The signer uses its private key to sign the URL or cookies, and CloudFront uses the public key to verify the signature. 
- When you create signed URLs or signed cookies, you use the private key from the signer’s key pair to sign a portion of the URL or the cookie
- When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account

- You must create the Lambda function from the same account as the container registry in Amazon ECR.

- Due to a spike in traffic, when Lambda functions scale, this causes the portion of requests that are served by new instances to have higher latency than the rest. To enable your function to scale without fluctuations in latency, *use provisioned concurrency*. By allocating provisioned concurrency before an increase in invocations, you can ensure that all requests are served by initialized instances with very low latency.

- To ensure that a function can always reach a certain level of concurrency, you can configure the function with reserved concurrency

- The maximum ratio of provisioned IOPS to requested volume size (in GiB) is 50:1

- UpdateItem action of DynamoDB APIs, edits an existing item's attributes or adds a new item to the table if it does not already exist