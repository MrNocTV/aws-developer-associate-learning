- Amazon EC2 Auto Scaling works with both Application Load Balancer and Network Load Balancer
- Amazon EC2 Auto Scaling cannot add a volume to an existing instance if the existing volume is approaching capacity.

- In the ecs.config file you have to configure the parameter ECS_CLUSTER='your_cluster_name' to register the container instance with a cluster named 'your_cluster_name'.

- Use Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects 
- Use Resource-based Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects
- Use Cross-account IAM roles for programmatic and console access to S3 bucket objects
- Stage Variables: Stage variables are name-value pairs that you can define as configuration attributes associated with a deployment stage of an API. They act like environment variables and can be used in your API setup and mapping templates.

- Amazon S3: If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent.

- Which section of cloud formation does not allow condition: parameters

- SQS max message size is 256KB.

- member accounts will be able to see the Organization trail, but cannot modify or delete it.
- by default, CloudTrail tracks only bucket-level actions, to track object-level actions, you need to enable Amazon S3 data events.
- A multi-national company maintains separate AWS accounts for different verticals in their organization. The project manager of a team wants to migrate the Elastic Beanstalk environment from Team A's AWS account into Team B's AWS account. As a Developer, you have been roped in to help him in this process: 
    -- create a saved configuration in Team A's account and download it to your local machine. Make the account-specific parameter changes and upload to the S3 bucket in Team B's account. From Elastic Beanstalk console, create an application from 'Saved Configurations'

- To use the exported value in another stack, use *!ImportValue*

!Ref - Returns the value of the specified parameter or resource.

!GetAtt - Returns the value of an attribute from a resource in the template.

!Sub - Substitutes variables in an input string with values that you specify.

!Join - This function appends a set of values into a single value, separated by the specified delimiter

- If you upload a local template file, AWS CloudFormation uploads it to an Amazon Simple Storage Service (Amazon S3) bucket in your AWS account. If you don't already have an S3 bucket that was created by AWS CloudFormation, it creates a unique bucket for each region in which you upload a template file

- CodeDeploy Deployment Groups : You can specify one or more deployment groups for a CodeDeploy application. The deployment group contains settings and configurations used during the deployment

- You can configure a Lambda function to connect to private subnets in a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC) to create a private network for resources such as databases, cache instances, or internal services. Connect your lambda function to the VPC to access private resources during execution. When you connect a function to a VPC, Lambda creates an elastic network interface for each combination of the security group and subnet in your function's VPC configuration

- Consider using Global tables if your application is accessed by globally distributed users - If you have globally dispersed users, consider using global tables. With global tables, you can specify the AWS Regions where you want the table to be available. This can significantly reduce latency for your users.

- During rollback which of the following instances did AWS CodeDeploy deploy first to: failed instances

- If you terminate a container instance while it is in the STOPPED state, that container instance isn't automatically removed from the cluster. You will need to deregister your container instance in the STOPPED state by using the Amazon ECS console or AWS Command Line Interface. Once deregistered, the container instance will no longer appear as a resource in your Amazon ECS cluster.

- By default, the health check configuration of your Auto Scaling group is set as an EC2 type that performs a status check of EC2 instances. To automate the replacement of unhealthy EC2 instances, you must change the health check type of your instance's Auto Scaling group from EC2 to ELB by using a configuration file.

- by default, scripts entered as user data are executed with root user privileges
- by default, user data runs only during the boot cycle when you first launch an instance

- assign permissions to services using IAM roles

- iam credentials report (account level), iam access advisor (user level)

- ec2 instance connect works only out of the box with Amazon Linux 2

- by default only the root EBS volume is deleted, any other attached EBS volume is not deleted

- not necessary to detach volume to do snapshot, but recommended.

- can copy snapshots across AZ or region

- AMI are built for a specific region (and like EBS, it can be copied across regions)

- EBS volume type: gp2/gp3 (general ssd), io1/io2 (high performance ssd), st1 (low cost hdd), sc1 (lowest cost hdd)

- only gp2/gp3 and io1/io2 can be used as boot volumes

- gp2/gp3 : PIOPS (Provisioned IOPS) increase along side with storage size
- io1/io2 : PIOPS can increase independently with storage size -> for applications that need more than 16000 IOPS

- *io1/io2 can be attached to multple EC2 instances in the same AZ*.

- uses security group to control access to EFS

- EFS is only compatible with Linux based AMI (not windows)

- EFS-IA (infrequently access)

- EBS backups use IO and you shouldn't run them while your application is handling a lot of traffic.

- you can setup private or public ELB

- ALB (application load balancer) supports load balancing to multiple applications on the same machine (containers), support Websocket and HTTP/2

- ALB has a port mapping feature to redirect to a dynamic port in ECS

- In comparison, we’d need multiple Classic Load Balancer per application

- ALB can route to multiple target groups, health checks are at the target group level

- NLB (Network Load Balancer) has one static IP per AZ , and supports assigning Elastic IP.

- stickiness works for CLB and ALB

- cross-zone load balancing: requests are distributed evenly across all registered instances in all AZ.

- cross-zone load balancing always on for ALB, enable by default through console for classic load balancer, disable by default for network load balancer

- load balancer use X.509 certificate

- SNI (Server Name Indication) to support multiple certs for multiple domains

- SNI solves the problem of loading multiple SSL certificates onto one web server, it's a "newer" protocol and requires the client to indicate the hostname of the target server in the initial SSL handshake

- SNI only works for newer genration load balancer (ALB, NLB)

- Connection draining (CLB) , deregistration delay (ALB, NLB) : time to complete in-flight requests while the instance is de-registering or unhealthy

- it's possible to scale an ASG based on CloudWatch Alarm

- it's now possible to define "better" auto scaling rules that are directly managed by EC2.

- ASG can terminate instances that are marked as unhealthy by LB (hence replace them)

- the cooldown period helps to ensure that your Auto Scaling group doesn't launch or terminate additional instances before the previous scaling activity takes effect.

- In AWS there’s a network cost when data goes from one AZ to another

- RDS read replica: async, RDS disaster recovery: sync

- If the master is not encrypted, the read replicas cannot be encrypted

- IAM-based authentication can be used to login into RDS MySQL & PostgreSQL (Aurora )

- RDS encryption at rest is done only when you first create the DB instance

- aurora writer endpoint, reader endpoint

• In AWS, the most common records are: 
• A: hostname to IPv4
• AAAA: hostname to IPv6
• CNAME: hostname to hostname (only works with non root domain)
• Alias: hostname to AWS resource. (works with root and non root domain)

- if multiple values are returned, a random one is chosen by the client

- NAT gateways & NAT Instances allow your instances in your Private Subnets to access the internet while remaining private

- Endpoints allow you to connect to AWS Services using a private network instead of the public www network

- buckets must have a globally unique name

- S3 max object size is 5TB, upload more than 5GB must use multipart upload

- any files that is not versioned prior to enabling versioning will have version "null"

- suspending versioning does not delete the previous versions

- There are 4 methods of encrypting objects in S3:
    + SSE-S3: keys handled and managed by S3, Must set header: “x-amz-server-side-encryption": "AES256"
    + SSE-KMS: keys handled and managed by KMS, Must set header: “x-amz-server-side-encryption": ”aws:kms"
    + SSE-C: key is managed by client, HTTPS must be used
    + Client side encryption

- User based: IAM
- Resources based: Bucket policies, Object access control list, bucket access control list
- use s3 bucket policy to :
    + grant public access to the bucket
    + force objects to be encrypted at upload
    + grant access to another account

- s3 access logs can be stored in other S3 bucket

• <bucket-name>.s3-website-<AWS-region>.amazonaws.com

- S3 strong consistency as of Dec 2020.

- never ever put your credentials on an EC2 using `aws configure`, assign IAM roles to EC2 instances instead.

- use --dry-run to check if we have the permissions to run the command.

- when you run API calls and they fail, you can get a long error message/

- this error message can be decoded using the STS commandline: *sts decode-authorization-message*

- EC2 Instance metadata: allows EC2 to learn about themselves without using an IAM role. http://169.254.169.254/latest/meta-data

- to use MFA with CLI, you must run STS GetSessionToken API call
- aws sts get-session-token --serial-number arn-of-the-mfa-device --token-code code-from-token --duration-seconds 3600

- when use SDK, if region is not set, it is *us-east-1* by default.

- You can request a service limit increase by opening a ticket

- You can request a service quota increase by using the Service Quotas API

- AWS CLI credentials chain:
    + command line options
    + environment variables
    + ~/.aws/credentials -> ~/.aws/config
    + container credentials (ec2 tasks)
    + instance profile credentials (ec2 instance profiles)

- AWS HTTP request should use Signature V4 (SigV4) for signing.

- If you use the SDK or CLI, the HTTP requests are signed for you.

- to use MFA, enable versioning on the S3 bucket.

- Only the bucket owner (root account) can enable/disable MFA-Delete

- you will need MFA to: permanently delete an object version, suspend versioning on the bucket.

- S3 replication is async, bucket can be in different regions, accounts.

- there is no chaining of replication, if bucket 1 has replication into bucket 2, which has replication into bucket 3, then objects created in bucket 1 are not replicated to bucket 3

- each item in Glacier is called Archive, archives are stored in Vaults

- 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket

- If you use SSE-KMS, you may be impacted by the KMS limits

-  When you upload, it calls the GenerateDataKey KMS API, When you download, it calls the Decrypt KMS 

- S3 transfer acceleration: Increase transfer speed by transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region

- S3 Byte-range fetch: Can be used to retrieve only partial data, Can be used to speed up downloads

- s3 select, glacier select: retrieve less data using SQL by performing server side filtering (simple SQL statements)

- S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer
- If two writes are made to a single non-versioned object at the same time, it is possible that only a single event notification will be sent
- If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket.

- AWS Athena: Serverless service to perform analytics directly against S3 files, Exam Tip: Analyze data directly on S3 => use Athena

- CloudFront origins: S3, ALB, EC2, S3 website, any HTTP backend you want

- CloudFront geo restriction: whitelist, blacklist

- Cloudfront: greate for static content that must be available everywhere
- S3 CRR : greate for dynamic content that needs to be available at low-latency in few regions.
- You can invalidate part of the cache using the CreateInvalidation API

- viewer protocol policy: redirect HTTP to HTTPS
- origin policy: match viewer (http - http, https - https) or https only
- s3 bucket websites dont support https.

- signed url: individual files
- signed cookies: multiple files

- in your cloudfront distribution, create one or more trusted key groups
- you generate your own private/public keys:
    + private key will be used by your application to sign URLs
    + public key will be used by CloudFront to verify URLs

- three price classes: all, 200, 100

- cloudfront origin groups: increase high-availability and do failover
- origin group: one primary and one secondary
- if the primary fails, the second one is used

- cloudfront field level encryption: uses asymmetric encryption

- the EC2 instances run a special AMI made specifically for ECS, it runs an ECS agent to reguster the instace with ECS cluster
- ECS task: tell ECS how to run a docker container
- ECS service: define how many tasks should be run and how they should be run   

- ECS service with LB and dynamic port forwarding

- AWS CLI v1 login command : $(aws ecr get-login --no-include-email --region eu-west-1)
- AWS CLI v2 login command : aws ecr get-login-password --region eu-west-1 | docker login --username AWS -- password-stdin 1234567890.dkr.ecr.eu-west-1.amazonaws.com

- Fargate: serverless, dont manage and provision EC2 instances, only need to create task definitions

- EC2 instance profile: used by ECS agent 
- ECS Task Role: allow each task to have specific role, use different roles for different service.

- ECS task placement strategy
    + instances match spec -> match constraints -> match strategy -> select instances
    + *distinctInstance*: place each task on a different container instance
    + *memberOf*: place task on instance satisfy an expression

- ECS task placement constraints

- ECS service scaling != EC2 Auto Scaling

- A Capacity Provider is used in association with a cluster to determine the infrastructure that a task runs on
- Fargate: FARGATE and FARGATE_SPOT capacity providers are added automatically
- ECS: you need to associate the capacity provider with an auto-scaling group

+ ECS + EBS: cant be shared
+ ECS + EFS: can be shared
+ Bind mounts: share data between containers of the same task, Great for “sidecar” container pattern where the sidecar can be used to send metrics/logs to other destinations

- We must configure the file */etc/ecs/ecs.config* with the cluster name
- In case an EC2 instance (or you) cannot pull a Docker image, check IAM
- Fargate tasks can have IAM Roles to execute actions against AWS

- Beanstalk is free but you pay for the underlying instances
- Beanstalks: application (with version) and environment name

- support multiple platforms: go, java, python,... if not supported, you can write your custom platform.

- beanstalk deployment options: all at once, rolling, rolling with additional batches, immutables (deployment time is sorted in ascending order, immutables is the longest one)

- blue/green deployment with route53 weighted policy and beanstalk's swap URL

- deployment process: describe dependency, zip code -> beanstalk will deploy zip code to ec2 instances, resolve dependencies and start the application

- beanstalk lifecycle policy: max 1000 app -> need to delete some, can set policy to delete, based on time (old versions) or space (too many versions)

- beanstalk extensions: *.ebextensions* directory in the root of source code, resources managed by `.ebextensions` get deleted if the environment goes away

- you can define CloudFormation resources in your .ebextensions to provision ElastiCache, an S3 bucket, anything you want

- beanstalk environment cloning, cannot clone load balancer though, so we have to do a trick by creating new environment and use a cname swap with route53

- beanstalk single docker: Dockerrun.aws.json (describe where the built docker image is)
- beanstalk multi docker: required Dockerrun.aws.json at the root of source code, it is used to generate ECS task definition

- beanstalk HTTPS:  .ebextensions/securelistener-alb.config

- beanstalk custom platform: define platform.yaml file and build the platform using Packer software (opensource tool for creating AMI)

- codepipeline: made of stages, each stage can have sequential actions and or parallel actions

- manual approval can be defined at any stage

- aws code deploy -> deploy to fleet of EC2 instances (not beanstalk, code pipeline can be deployed to code deploy or beanstalk)

- each pipeline stage create an artifact, artifact go to S3 and be input of next stage

- codebuild instruction is defined in *buildspec.yaml* file, must be at the root of your code
    + 4 phases: install, prebuild, build, postbuild -> artifacts (to S3)

- codebuild can be run locally for troubleshooting

- builds can be defined within codepipeline or codebuild itself.

- by default, codebuild are launched outside your VPC -> cannot access resources in a VPC -> you can specify a VPC configuration

- code deploy : *appspec.yaml*, ec2 machine must be running code deploy agent

- EC2 instances are grouped by deployment group

- code deploy does not provision resources

- code deploy appspec.yaml hooks: App stop, download bundle, before install, after install, app start, validate service

- code deploy failed: instances will be at failed state, new deployments will first be deployed to failed state instances

- If a roll back happens, CodeDeploy redeploys the last known good revision as a new deployment.

- cloudformation stacks: separate of concern, vpc stacks, network stacks, app stacks

- cloudformation templates have to be uploaded in S3 and then referenced in cloudformation

- cant upload a cloudformation template, have to create a new one

- cloudformation template components: 
    + resources: aws resources (MANDATORY)
    + parameters: dynamic inputs
    + mapping: static variables
    + outputs: declares optional outputs values that we can import into other stacks (if you export them first)
    + conditions: used to control the creation of resources or outputs based on a condition
    + metadata


- aws resources form: AWS::aws-product-name::data-type-name

- you cant delete a cloudformation stack if its outputs are being referenced by another cloudformation stack

- !FindInMap [ MapName, TopLevelKey, SecondLevelKey ]

- Fn::GetAtt : get attributes attached to any resources you create

- nested stack: separate common components

- cloudformation stackset: CRUD for stacks across multiple accounts and regions with a single operation

- cloudformation drift: if you want to know if the infrastructure is changed manually, use cloudformation drift